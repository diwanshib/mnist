{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST best results\n",
    "Result\tMethod\tVenue\tDetails\n",
    "1. 0.21%\tRegularization of Neural Networks using DropConnect\tICML 2013\t\n",
    "2. 0.23%\tMulti-column Deep Neural Networks for Image ClassiÔ¨Åcation\tCVPR 2012\t\n",
    "3. 0.23%\tAPAC: Augmented PAttern Classification with Neural Networks\tarXiv 2015\t\n",
    "4. 0.24%\tBatch-normalized Maxout Network in Network\tarXiv 2015\tDetails\n",
    "5. 0.29%\tGeneralizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree\tAISTATS 2016\tDetails\n",
    "6. 0.31%\tRecurrent Convolutional Neural Network for Object Recognition\tCVPR 2015\t\n",
    "\n",
    "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1070 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Lambda, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam as Adam\n",
    "\n",
    "# try leaky relu\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = 28\n",
    "cols = 28\n",
    "\n",
    "# theano input shape\n",
    "input_shape = (1, rows, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vgg expects channels - here we have just one\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to float\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "# normalize\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels should be onehot encoded\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate mean and standard deviation\n",
    "mean_px = x_train.mean().astype(np.float32)\n",
    "std_px = x_train.std().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to normalize input data\n",
    "def norm_input(x): return (x-mean_px)/std_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jeremy's model, inspired by vgg16\n",
    "# Batchnorm + dropout + data augmentation\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1,28,28), output_shape=(1,28,28)),\n",
    "        Conv2D(32, (3,3)),\n",
    "        LeakyReLU(),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(32, (3,3)),\n",
    "        LeakyReLU(),\n",
    "        MaxPooling2D(),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(64, (3,3)),\n",
    "        LeakyReLU(),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(64, (3,3)),\n",
    "        LeakyReLU(),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dense(512),\n",
    "        LeakyReLU(),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "Use keras's data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=12, width_shift_range=0.1, shear_range=0.3,\n",
    "                        height_shift_range=0.1, zoom_range=0.1, data_format='channels_first')\n",
    "batches = gen.flow(x_train, y_train, batch_size=batch_size)\n",
    "test_batches = gen.flow(x_test, y_test, batch_size=batch_size)\n",
    "steps_per_epoch = int(np.ceil(batches.n/batch_size))\n",
    "validation_steps = int(np.ceil(test_batches.n/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f06a0c71a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+5Ke6Lp0ZxEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f06a2cf9d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load ONE image from training set to display on screen\n",
    "img = x_train[1]\n",
    "\n",
    "# visualize original image\n",
    "plt.imshow(img[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trick our generator into believing img has enough dimensions\n",
    "# and get some augmented images for our single test image\n",
    "img = np.expand_dims(img, axis=0)\n",
    "aug_iter = gen.flow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_img = next(aug_iter)[0].astype(np.float32)\n",
    "aug_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAABqCAYAAABZAFxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f06a0cad208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show augmented images\n",
    "f = plt.figure(figsize=(12,6))\n",
    "for i in range(8):\n",
    "    sp = f.add_subplot(2, 26//3, i+1)\n",
    "    sp.axis('Off')\n",
    "    aug_img = next(aug_iter)[0].astype(np.float32)\n",
    "    plt.imshow(aug_img[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "# Only required if featurewise_center or featurewise_std_normalization or zca_whitening.\n",
    "# gen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models with pretrained weights\n",
    "Here we build on previously trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/js/Dokumente/mnist'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights for existing models and train them some more\n",
    "# this will hopefully result in a better prediction accuracy\n",
    "models = []\n",
    "for i in range(10):\n",
    "    m = create_model()\n",
    "    m.load_weights(\"weights/weights_leaky_relu_model_0.23_\"+str(i)+'.pkl')    \n",
    "    models.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 11s - loss: 0.0201 - acc: 0.9937 - val_loss: 0.0198 - val_acc: 0.9936\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 11s - loss: 0.0208 - acc: 0.9936 - val_loss: 0.0206 - val_acc: 0.9941\n",
      "Epoch 3/16\n",
      "118/118 [==============================] - 11s - loss: 0.0191 - acc: 0.9938 - val_loss: 0.0209 - val_acc: 0.9936\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 11s - loss: 0.0212 - acc: 0.9933 - val_loss: 0.0197 - val_acc: 0.9936\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0179 - acc: 0.9942 - val_loss: 0.0219 - val_acc: 0.9925\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9939 - val_loss: 0.0195 - val_acc: 0.9934\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 11s - loss: 0.0192 - acc: 0.9939 - val_loss: 0.0178 - val_acc: 0.9942\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 11s - loss: 0.0183 - acc: 0.9937 - val_loss: 0.0211 - val_acc: 0.9935\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 10s - loss: 0.0189 - acc: 0.9939 - val_loss: 0.0192 - val_acc: 0.9929\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 10s - loss: 0.0202 - acc: 0.9935 - val_loss: 0.0220 - val_acc: 0.9929\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 11s - loss: 0.0182 - acc: 0.9944 - val_loss: 0.0205 - val_acc: 0.9933\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0190 - acc: 0.9938 - val_loss: 0.0199 - val_acc: 0.9928\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 10s - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0223 - val_acc: 0.9937\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 10s - loss: 0.0182 - acc: 0.9940 - val_loss: 0.0216 - val_acc: 0.9937\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 10s - loss: 0.0186 - acc: 0.9942 - val_loss: 0.0189 - val_acc: 0.9934\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 11s - loss: 0.0190 - acc: 0.9942 - val_loss: 0.0200 - val_acc: 0.9933\n",
      "Model 1\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 11s - loss: 0.0199 - acc: 0.9935 - val_loss: 0.0206 - val_acc: 0.9937\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 11s - loss: 0.0216 - acc: 0.9934 - val_loss: 0.0198 - val_acc: 0.9930\n",
      "Epoch 3/16\n",
      "118/118 [==============================] - 11s - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0201 - val_acc: 0.9927\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9937 - val_loss: 0.0203 - val_acc: 0.9946\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0211 - val_acc: 0.9930\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0215 - val_acc: 0.9933\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 11s - loss: 0.0193 - acc: 0.9938 - val_loss: 0.0159 - val_acc: 0.9946\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 11s - loss: 0.0192 - acc: 0.9939 - val_loss: 0.0210 - val_acc: 0.9935\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 11s - loss: 0.0214 - acc: 0.9931 - val_loss: 0.0201 - val_acc: 0.9940\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 11s - loss: 0.0187 - acc: 0.9939 - val_loss: 0.0202 - val_acc: 0.9935\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 11s - loss: 0.0200 - acc: 0.9932 - val_loss: 0.0214 - val_acc: 0.9928\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0191 - acc: 0.9942 - val_loss: 0.0169 - val_acc: 0.9942\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 11s - loss: 0.0207 - acc: 0.9935 - val_loss: 0.0196 - val_acc: 0.9936\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 11s - loss: 0.0194 - acc: 0.9942 - val_loss: 0.0216 - val_acc: 0.9932\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 11s - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0198 - val_acc: 0.9935\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 11s - loss: 0.0202 - acc: 0.9938 - val_loss: 0.0206 - val_acc: 0.9932\n",
      "Model 2\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 12s - loss: 0.0196 - acc: 0.9939 - val_loss: 0.0246 - val_acc: 0.9917\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 11s - loss: 0.0208 - acc: 0.9934 - val_loss: 0.0195 - val_acc: 0.9943\n",
      "Epoch 3/16\n",
      "118/118 [==============================] - 11s - loss: 0.0188 - acc: 0.9937 - val_loss: 0.0203 - val_acc: 0.9928\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 11s - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0231 - val_acc: 0.9930\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0197 - acc: 0.9940 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0187 - acc: 0.9940 - val_loss: 0.0215 - val_acc: 0.9932\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 11s - loss: 0.0201 - acc: 0.9937 - val_loss: 0.0215 - val_acc: 0.9934\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 11s - loss: 0.0209 - acc: 0.9932 - val_loss: 0.0213 - val_acc: 0.9924\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0200 - val_acc: 0.9933\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9936 - val_loss: 0.0192 - val_acc: 0.9939\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 11s - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0205 - val_acc: 0.9937\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0196 - acc: 0.9938 - val_loss: 0.0207 - val_acc: 0.9935\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 11s - loss: 0.0203 - acc: 0.9938 - val_loss: 0.0229 - val_acc: 0.9924\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 11s - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0250 - val_acc: 0.9918\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 11s - loss: 0.0198 - acc: 0.9935 - val_loss: 0.0220 - val_acc: 0.9936\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 11s - loss: 0.0199 - acc: 0.9939 - val_loss: 0.0183 - val_acc: 0.9937\n",
      "Model 3\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 12s - loss: 0.0193 - acc: 0.9937 - val_loss: 0.0196 - val_acc: 0.9937\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 11s - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0194 - val_acc: 0.9929\n",
      "Epoch 3/16\n",
      "118/118 [==============================] - 11s - loss: 0.0186 - acc: 0.9937 - val_loss: 0.0201 - val_acc: 0.9923\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 11s - loss: 0.0205 - acc: 0.9938 - val_loss: 0.0211 - val_acc: 0.9934\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0199 - acc: 0.9933 - val_loss: 0.0204 - val_acc: 0.9936\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0187 - acc: 0.9937 - val_loss: 0.0226 - val_acc: 0.9933\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 11s - loss: 0.0197 - acc: 0.9935 - val_loss: 0.0186 - val_acc: 0.9935\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 11s - loss: 0.0193 - acc: 0.9938 - val_loss: 0.0200 - val_acc: 0.9941\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 11s - loss: 0.0192 - acc: 0.9941 - val_loss: 0.0201 - val_acc: 0.9937\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 11s - loss: 0.0199 - acc: 0.9934 - val_loss: 0.0214 - val_acc: 0.9931\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 11s - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0198 - val_acc: 0.9930\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9937 - val_loss: 0.0215 - val_acc: 0.9926\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 11s - loss: 0.0187 - acc: 0.9943 - val_loss: 0.0204 - val_acc: 0.9933\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 11s - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0181 - val_acc: 0.9940\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9937 - val_loss: 0.0200 - val_acc: 0.9939\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 11s - loss: 0.0202 - acc: 0.9939 - val_loss: 0.0213 - val_acc: 0.9931\n",
      "Model 4\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 11s - loss: 0.0194 - acc: 0.9938 - val_loss: 0.0226 - val_acc: 0.9929\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 10s - loss: 0.0196 - acc: 0.9938 - val_loss: 0.0212 - val_acc: 0.9929\n",
      "Epoch 3/16\n",
      "118/118 [==============================] - 11s - loss: 0.0208 - acc: 0.9936 - val_loss: 0.0216 - val_acc: 0.9939\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 10s - loss: 0.0201 - acc: 0.9940 - val_loss: 0.0211 - val_acc: 0.9931\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0198 - acc: 0.9938 - val_loss: 0.0238 - val_acc: 0.9919\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0187 - acc: 0.9939 - val_loss: 0.0197 - val_acc: 0.9937\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 11s - loss: 0.0183 - acc: 0.9941 - val_loss: 0.0219 - val_acc: 0.9926\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 11s - loss: 0.0198 - acc: 0.9937 - val_loss: 0.0198 - val_acc: 0.9933\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 11s - loss: 0.0194 - acc: 0.9936 - val_loss: 0.0196 - val_acc: 0.9941\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 11s - loss: 0.0197 - acc: 0.9938 - val_loss: 0.0215 - val_acc: 0.9926\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 11s - loss: 0.0203 - acc: 0.9937 - val_loss: 0.0201 - val_acc: 0.9935\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0198 - val_acc: 0.9933\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 11s - loss: 0.0189 - acc: 0.9939 - val_loss: 0.0179 - val_acc: 0.9933\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 10s - loss: 0.0203 - acc: 0.9936 - val_loss: 0.0208 - val_acc: 0.9939\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 10s - loss: 0.0198 - acc: 0.9941 - val_loss: 0.0257 - val_acc: 0.9917\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 10s - loss: 0.0187 - acc: 0.9939 - val_loss: 0.0182 - val_acc: 0.9936\n",
      "Model 5\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 11s - loss: 0.0207 - acc: 0.9932 - val_loss: 0.0246 - val_acc: 0.9921\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 11s - loss: 0.0185 - acc: 0.9943 - val_loss: 0.0185 - val_acc: 0.9940\n",
      "Epoch 3/16\n",
      "118/118 [==============================] - 11s - loss: 0.0193 - acc: 0.9938 - val_loss: 0.0218 - val_acc: 0.9939\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 10s - loss: 0.0209 - acc: 0.9938 - val_loss: 0.0208 - val_acc: 0.9933\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0183 - acc: 0.9940 - val_loss: 0.0201 - val_acc: 0.9938\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0189 - acc: 0.9939 - val_loss: 0.0187 - val_acc: 0.9944\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 11s - loss: 0.0201 - acc: 0.9932 - val_loss: 0.0212 - val_acc: 0.9928\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 11s - loss: 0.0187 - acc: 0.9939 - val_loss: 0.0196 - val_acc: 0.9935\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 10s - loss: 0.0199 - acc: 0.9938 - val_loss: 0.0200 - val_acc: 0.9934\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 11s - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0195 - val_acc: 0.9936\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 10s - loss: 0.0208 - acc: 0.9933 - val_loss: 0.0218 - val_acc: 0.9938\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0198 - acc: 0.9938 - val_loss: 0.0168 - val_acc: 0.9938\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 11s - loss: 0.0215 - acc: 0.9932 - val_loss: 0.0211 - val_acc: 0.9938\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 11s - loss: 0.0202 - acc: 0.9934 - val_loss: 0.0168 - val_acc: 0.9942\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9937 - val_loss: 0.0201 - val_acc: 0.9932\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 11s - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0178 - val_acc: 0.9944\n",
      "Model 6\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 11s - loss: 0.0213 - acc: 0.9929 - val_loss: 0.0249 - val_acc: 0.9926\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 10s - loss: 0.0215 - acc: 0.9936 - val_loss: 0.0206 - val_acc: 0.9929\n",
      "Epoch 3/16\n",
      "118/118 [==============================] - 11s - loss: 0.0190 - acc: 0.9938 - val_loss: 0.0211 - val_acc: 0.9935\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 11s - loss: 0.0207 - acc: 0.9934 - val_loss: 0.0205 - val_acc: 0.9931\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0197 - acc: 0.9937 - val_loss: 0.0194 - val_acc: 0.9930\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0203 - acc: 0.9937 - val_loss: 0.0213 - val_acc: 0.9934\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 11s - loss: 0.0200 - acc: 0.9931 - val_loss: 0.0199 - val_acc: 0.9934\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 11s - loss: 0.0189 - acc: 0.9942 - val_loss: 0.0209 - val_acc: 0.9928\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 11s - loss: 0.0218 - acc: 0.9934 - val_loss: 0.0184 - val_acc: 0.9946\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 11s - loss: 0.0193 - acc: 0.9942 - val_loss: 0.0231 - val_acc: 0.9924\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 11s - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0228 - val_acc: 0.9924\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0204 - acc: 0.9938 - val_loss: 0.0201 - val_acc: 0.9932\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 11s - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0200 - val_acc: 0.9936\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 11s - loss: 0.0207 - acc: 0.9934 - val_loss: 0.0251 - val_acc: 0.9921\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 11s - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0186 - val_acc: 0.9936\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 11s - loss: 0.0212 - acc: 0.9931 - val_loss: 0.0223 - val_acc: 0.9925\n",
      "Model 7\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 11s - loss: 0.0187 - acc: 0.9937 - val_loss: 0.0208 - val_acc: 0.9930\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 11s - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0227 - val_acc: 0.9926\n",
      "Epoch 3/16\n",
      "118/118 [==============================] - 11s - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0231 - val_acc: 0.9927\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 11s - loss: 0.0208 - acc: 0.9934 - val_loss: 0.0192 - val_acc: 0.9934\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0180 - acc: 0.9941 - val_loss: 0.0189 - val_acc: 0.9938\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0214 - acc: 0.9932 - val_loss: 0.0191 - val_acc: 0.9934\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 11s - loss: 0.0175 - acc: 0.9943 - val_loss: 0.0207 - val_acc: 0.9934\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0192 - val_acc: 0.9932\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 11s - loss: 0.0183 - acc: 0.9939 - val_loss: 0.0207 - val_acc: 0.9940\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 11s - loss: 0.0224 - acc: 0.9932 - val_loss: 0.0186 - val_acc: 0.9937\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 11s - loss: 0.0188 - acc: 0.9939 - val_loss: 0.0232 - val_acc: 0.9930\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0201 - acc: 0.9935 - val_loss: 0.0185 - val_acc: 0.9940\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 11s - loss: 0.0212 - acc: 0.9931 - val_loss: 0.0204 - val_acc: 0.9935\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9936 - val_loss: 0.0181 - val_acc: 0.9933\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 11s - loss: 0.0197 - acc: 0.9933 - val_loss: 0.0198 - val_acc: 0.9929\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 11s - loss: 0.0190 - acc: 0.9938 - val_loss: 0.0219 - val_acc: 0.9933\n",
      "Model 8\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 11s - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0214 - val_acc: 0.9941\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 11s - loss: 0.0196 - acc: 0.9935 - val_loss: 0.0192 - val_acc: 0.9936\n",
      "Epoch 3/16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 11s - loss: 0.0201 - acc: 0.9935 - val_loss: 0.0176 - val_acc: 0.9942\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 11s - loss: 0.0196 - acc: 0.9938 - val_loss: 0.0186 - val_acc: 0.9937\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0186 - acc: 0.9941 - val_loss: 0.0226 - val_acc: 0.9929\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0196 - acc: 0.9936 - val_loss: 0.0192 - val_acc: 0.9936\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 10s - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0169 - val_acc: 0.9950\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 10s - loss: 0.0196 - acc: 0.9941 - val_loss: 0.0207 - val_acc: 0.9924\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 10s - loss: 0.0204 - acc: 0.9936 - val_loss: 0.0223 - val_acc: 0.9939\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0191 - val_acc: 0.9941\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 11s - loss: 0.0194 - acc: 0.9938 - val_loss: 0.0228 - val_acc: 0.9930\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0197 - acc: 0.9939 - val_loss: 0.0244 - val_acc: 0.9918\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 11s - loss: 0.0196 - acc: 0.9941 - val_loss: 0.0215 - val_acc: 0.9930\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 11s - loss: 0.0204 - acc: 0.9937 - val_loss: 0.0199 - val_acc: 0.9932\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 11s - loss: 0.0192 - acc: 0.9938 - val_loss: 0.0198 - val_acc: 0.9930\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 11s - loss: 0.0202 - acc: 0.9938 - val_loss: 0.0203 - val_acc: 0.9936\n",
      "Model 9\n",
      "Epoch 1/16\n",
      "118/118 [==============================] - 11s - loss: 0.0196 - acc: 0.9940 - val_loss: 0.0179 - val_acc: 0.9937\n",
      "Epoch 2/16\n",
      "118/118 [==============================] - 11s - loss: 0.0186 - acc: 0.9940 - val_loss: 0.0207 - val_acc: 0.9934\n",
      "Epoch 3/16\n",
      "118/118 [==============================] - 10s - loss: 0.0183 - acc: 0.9940 - val_loss: 0.0206 - val_acc: 0.9926\n",
      "Epoch 4/16\n",
      "118/118 [==============================] - 11s - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0168 - val_acc: 0.9941\n",
      "Epoch 5/16\n",
      "118/118 [==============================] - 11s - loss: 0.0188 - acc: 0.9941 - val_loss: 0.0221 - val_acc: 0.9928\n",
      "Epoch 6/16\n",
      "118/118 [==============================] - 11s - loss: 0.0184 - acc: 0.9943 - val_loss: 0.0238 - val_acc: 0.9929\n",
      "Epoch 7/16\n",
      "118/118 [==============================] - 10s - loss: 0.0189 - acc: 0.9941 - val_loss: 0.0192 - val_acc: 0.9931\n",
      "Epoch 8/16\n",
      "118/118 [==============================] - 11s - loss: 0.0198 - acc: 0.9937 - val_loss: 0.0211 - val_acc: 0.9934\n",
      "Epoch 9/16\n",
      "118/118 [==============================] - 10s - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0217 - val_acc: 0.9930\n",
      "Epoch 10/16\n",
      "118/118 [==============================] - 11s - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0182 - val_acc: 0.9936\n",
      "Epoch 11/16\n",
      "118/118 [==============================] - 11s - loss: 0.0182 - acc: 0.9943 - val_loss: 0.0196 - val_acc: 0.9939\n",
      "Epoch 12/16\n",
      "118/118 [==============================] - 11s - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0211 - val_acc: 0.9934\n",
      "Epoch 13/16\n",
      "118/118 [==============================] - 10s - loss: 0.0197 - acc: 0.9936 - val_loss: 0.0233 - val_acc: 0.9931\n",
      "Epoch 14/16\n",
      "118/118 [==============================] - 11s - loss: 0.0191 - acc: 0.9936 - val_loss: 0.0183 - val_acc: 0.9940\n",
      "Epoch 15/16\n",
      "118/118 [==============================] - 10s - loss: 0.0192 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9932\n",
      "Epoch 16/16\n",
      "118/118 [==============================] - 11s - loss: 0.0200 - acc: 0.9939 - val_loss: 0.0215 - val_acc: 0.9929\n"
     ]
    }
   ],
   "source": [
    "# train models some more if accuracy is not satisfying enough\n",
    "for i, m in enumerate(models):\n",
    "    print(\"Model {}\".format(i))\n",
    "    m.optimizer.lr = 0.000001\n",
    "    m.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=16,\n",
    "                   validation_data=test_batches, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling\n",
    "Here we create ten models and fit them to our augmented data. We use the mean prediction from all ten classifiers as our final prediction. This should average out errors in our classifiers and enable us to predict with higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model on batches with real-time data augmentation\n",
    "def fit_model(m):\n",
    "    m.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=1, verbose=0,\n",
    "                   validation_data=test_batches, validation_steps=validation_steps)\n",
    "    m.optimizer.lr = 0.1\n",
    "    m.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=4, verbose=0,\n",
    "                   validation_data=test_batches, validation_steps=validation_steps)\n",
    "    m.optimizer.lr = 0.01\n",
    "    m.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=12, verbose=0,\n",
    "                   validation_data=test_batches, validation_steps=validation_steps)\n",
    "    m.optimizer.lr = 0.001\n",
    "    m.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=18, verbose=0,\n",
    "                   validation_data=test_batches, validation_steps=validation_steps)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train ten models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Training model {}\".format(i))\n",
    "    m = fit_model(create_model())\n",
    "    models.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0249 - acc: 0.9922 - val_loss: 0.0241 - val_acc: 0.9925\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0263 - acc: 0.9919 - val_loss: 0.0227 - val_acc: 0.9924\n",
      "Model 1\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0276 - acc: 0.9917 - val_loss: 0.0293 - val_acc: 0.9911\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0277 - acc: 0.9916 - val_loss: 0.0251 - val_acc: 0.9910\n",
      "Model 2\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0272 - acc: 0.9912 - val_loss: 0.0265 - val_acc: 0.9917\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0254 - acc: 0.9920 - val_loss: 0.0262 - val_acc: 0.9917\n",
      "Model 3\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0256 - acc: 0.9918 - val_loss: 0.0270 - val_acc: 0.9917\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0272 - acc: 0.9912 - val_loss: 0.0260 - val_acc: 0.9915\n",
      "Model 4\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0265 - acc: 0.9915 - val_loss: 0.0267 - val_acc: 0.9917\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0234 - val_acc: 0.9929\n",
      "Model 5\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0278 - acc: 0.9911 - val_loss: 0.0284 - val_acc: 0.9899\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0275 - acc: 0.9911 - val_loss: 0.0261 - val_acc: 0.9917\n",
      "Model 6\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0265 - acc: 0.9918 - val_loss: 0.0253 - val_acc: 0.9917\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0245 - acc: 0.9921 - val_loss: 0.0234 - val_acc: 0.9925\n",
      "Model 7\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0269 - acc: 0.9915 - val_loss: 0.0279 - val_acc: 0.9907\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0255 - acc: 0.9916 - val_loss: 0.0245 - val_acc: 0.9925\n",
      "Model 8\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0260 - acc: 0.9917 - val_loss: 0.0262 - val_acc: 0.9915\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0241 - acc: 0.9923 - val_loss: 0.0250 - val_acc: 0.9923\n",
      "Model 9\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 11s - loss: 0.0275 - acc: 0.9916 - val_loss: 0.0226 - val_acc: 0.9923\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 10s - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0292 - val_acc: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# train models some more if accuracy is not satisfying enough\n",
    "for i, m in enumerate(models):\n",
    "    m.optimizer.lr = 0.0001\n",
    "    print(\"Model {}\".format(i))\n",
    "    m.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=2,\n",
    "                   validation_data=test_batches, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9728/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# evaluate every model\n",
    "evals = np.array([m.evaluate(x_test,y_test, batch_size=eval_batch_size) for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01122509,  0.99641   ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean from evaluations\n",
    "evals.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = np.stack([m.predict(x_test, batch_size=eval_batch_size) for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds = all_preds.mean(axis=0)\n",
    "avg_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23000240325927734"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display error rate in percent\n",
    "(1 - keras.metrics.categorical_accuracy(y_test, avg_preds).eval().mean()) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Activation \"relu\":\n",
    "1. 0.26999711990356445\n",
    "2. 0.26000142097473145 trained for one additional epoch\n",
    "\n",
    "Activation \"leaky relu\":\n",
    "1. 0.34000277519226074\n",
    "2. 0.23999810218811035 + two epochs with lr 0.0001 (saved weights as _24)\n",
    "3. 0.23000240325927734 + two epochs with lr 0.0001 (saved weights as _23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try evaluation only with best performing models\n",
    "This doesn't seem to improve results at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_numbers = [0,1,2,3,4,5,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_models = []\n",
    "\n",
    "for i in best_model_numbers:\n",
    "    best_models.append(models[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9728/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# evaluate every model\n",
    "evals = np.array([m.evaluate(x_test,y_test, batch_size=eval_batch_size) for m in best_models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01328905,  0.9958125 ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean from evaluations\n",
    "evals.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = np.stack([m.predict(x_test, batch_size=eval_batch_size) for m in best_models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10000, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds = all_preds.mean(axis=0)\n",
    "avg_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34000277519226074"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display error rate in percent\n",
    "(1 - keras.metrics.categorical_accuracy(y_test, avg_preds).eval().mean()) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_15 (Lambda)           (None, 1, 28, 28)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 32, 26, 26)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 32, 26, 26)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 32, 26, 26)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 32, 24, 24)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 32, 24, 24)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 32, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 32, 12, 12)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 64, 10, 10)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 64, 10, 10)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 64, 10, 10)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 64, 8, 8)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)   (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 64, 4, 4)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 601,578.0\n",
      "Trainable params: 598,250.0\n",
      "Non-trainable params: 3,328.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1578pt\" viewBox=\"0.00 0.00 274.00 1578.00\" width=\"274pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1574)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1574 270,-1574 270,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139666159284744 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139666159284744</title>\n",
       "<polygon fill=\"none\" points=\"43.5,-1533.5 43.5,-1569.5 222.5,-1569.5 222.5,-1533.5 43.5,-1533.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-1547.8\">lambda_15_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139666159636264 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139666159636264</title>\n",
       "<polygon fill=\"none\" points=\"68,-1460.5 68,-1496.5 198,-1496.5 198,-1460.5 68,-1460.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-1474.8\">lambda_15: Lambda</text>\n",
       "</g>\n",
       "<!-- 139666159284744&#45;&gt;139666159636264 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139666159284744-&gt;139666159636264</title>\n",
       "<path d=\"M133,-1533.31C133,-1525.29 133,-1515.55 133,-1506.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-1506.53 133,-1496.53 129.5,-1506.53 136.5,-1506.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159252592 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139666159252592</title>\n",
       "<polygon fill=\"none\" points=\"68,-1387.5 68,-1423.5 198,-1423.5 198,-1387.5 68,-1387.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-1401.8\">conv2d_57: Conv2D</text>\n",
       "</g>\n",
       "<!-- 139666159636264&#45;&gt;139666159252592 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139666159636264-&gt;139666159252592</title>\n",
       "<path d=\"M133,-1460.31C133,-1452.29 133,-1442.55 133,-1433.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-1433.53 133,-1423.53 129.5,-1433.53 136.5,-1433.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159290128 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139666159290128</title>\n",
       "<polygon fill=\"none\" points=\"45.5,-1314.5 45.5,-1350.5 220.5,-1350.5 220.5,-1314.5 45.5,-1314.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-1328.8\">leaky_re_lu_71: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 139666159252592&#45;&gt;139666159290128 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139666159252592-&gt;139666159290128</title>\n",
       "<path d=\"M133,-1387.31C133,-1379.29 133,-1369.55 133,-1360.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-1360.53 133,-1350.53 129.5,-1360.53 136.5,-1360.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159288504 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139666159288504</title>\n",
       "<polygon fill=\"none\" points=\"0,-1241.5 0,-1277.5 266,-1277.5 266,-1241.5 0,-1241.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-1255.8\">batch_normalization_71: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139666159290128&#45;&gt;139666159288504 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139666159290128-&gt;139666159288504</title>\n",
       "<path d=\"M133,-1314.31C133,-1306.29 133,-1296.55 133,-1287.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-1287.53 133,-1277.53 129.5,-1287.53 136.5,-1287.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159288728 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139666159288728</title>\n",
       "<polygon fill=\"none\" points=\"68,-1168.5 68,-1204.5 198,-1204.5 198,-1168.5 68,-1168.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-1182.8\">conv2d_58: Conv2D</text>\n",
       "</g>\n",
       "<!-- 139666159288504&#45;&gt;139666159288728 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139666159288504-&gt;139666159288728</title>\n",
       "<path d=\"M133,-1241.31C133,-1233.29 133,-1223.55 133,-1214.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-1214.53 133,-1204.53 129.5,-1214.53 136.5,-1214.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159289064 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139666159289064</title>\n",
       "<polygon fill=\"none\" points=\"45.5,-1095.5 45.5,-1131.5 220.5,-1131.5 220.5,-1095.5 45.5,-1095.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-1109.8\">leaky_re_lu_72: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 139666159288728&#45;&gt;139666159289064 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139666159288728-&gt;139666159289064</title>\n",
       "<path d=\"M133,-1168.31C133,-1160.29 133,-1150.55 133,-1141.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-1141.53 133,-1131.53 129.5,-1141.53 136.5,-1141.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159289232 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139666159289232</title>\n",
       "<polygon fill=\"none\" points=\"26,-1022.5 26,-1058.5 240,-1058.5 240,-1022.5 26,-1022.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-1036.8\">max_pooling2d_29: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 139666159289064&#45;&gt;139666159289232 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139666159289064-&gt;139666159289232</title>\n",
       "<path d=\"M133,-1095.31C133,-1087.29 133,-1077.55 133,-1068.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-1068.53 133,-1058.53 129.5,-1068.53 136.5,-1068.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159289512 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139666159289512</title>\n",
       "<polygon fill=\"none\" points=\"0,-949.5 0,-985.5 266,-985.5 266,-949.5 0,-949.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-963.8\">batch_normalization_72: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139666159289232&#45;&gt;139666159289512 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>139666159289232-&gt;139666159289512</title>\n",
       "<path d=\"M133,-1022.31C133,-1014.29 133,-1004.55 133,-995.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-995.529 133,-985.529 129.5,-995.529 136.5,-995.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159289736 -->\n",
       "<g class=\"node\" id=\"node10\"><title>139666159289736</title>\n",
       "<polygon fill=\"none\" points=\"68,-876.5 68,-912.5 198,-912.5 198,-876.5 68,-876.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-890.8\">conv2d_59: Conv2D</text>\n",
       "</g>\n",
       "<!-- 139666159289512&#45;&gt;139666159289736 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>139666159289512-&gt;139666159289736</title>\n",
       "<path d=\"M133,-949.313C133,-941.289 133,-931.547 133,-922.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-922.529 133,-912.529 129.5,-922.529 136.5,-922.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159290072 -->\n",
       "<g class=\"node\" id=\"node11\"><title>139666159290072</title>\n",
       "<polygon fill=\"none\" points=\"45.5,-803.5 45.5,-839.5 220.5,-839.5 220.5,-803.5 45.5,-803.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-817.8\">leaky_re_lu_73: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 139666159289736&#45;&gt;139666159290072 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>139666159289736-&gt;139666159290072</title>\n",
       "<path d=\"M133,-876.313C133,-868.289 133,-858.547 133,-849.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-849.529 133,-839.529 129.5,-849.529 136.5,-849.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159290408 -->\n",
       "<g class=\"node\" id=\"node12\"><title>139666159290408</title>\n",
       "<polygon fill=\"none\" points=\"0,-730.5 0,-766.5 266,-766.5 266,-730.5 0,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-744.8\">batch_normalization_73: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139666159290072&#45;&gt;139666159290408 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>139666159290072-&gt;139666159290408</title>\n",
       "<path d=\"M133,-803.313C133,-795.289 133,-785.547 133,-776.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-776.529 133,-766.529 129.5,-776.529 136.5,-776.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159290688 -->\n",
       "<g class=\"node\" id=\"node13\"><title>139666159290688</title>\n",
       "<polygon fill=\"none\" points=\"68,-657.5 68,-693.5 198,-693.5 198,-657.5 68,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-671.8\">conv2d_60: Conv2D</text>\n",
       "</g>\n",
       "<!-- 139666159290408&#45;&gt;139666159290688 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>139666159290408-&gt;139666159290688</title>\n",
       "<path d=\"M133,-730.313C133,-722.289 133,-712.547 133,-703.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-703.529 133,-693.529 129.5,-703.529 136.5,-703.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159291024 -->\n",
       "<g class=\"node\" id=\"node14\"><title>139666159291024</title>\n",
       "<polygon fill=\"none\" points=\"45.5,-584.5 45.5,-620.5 220.5,-620.5 220.5,-584.5 45.5,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-598.8\">leaky_re_lu_74: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 139666159290688&#45;&gt;139666159291024 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>139666159290688-&gt;139666159291024</title>\n",
       "<path d=\"M133,-657.313C133,-649.289 133,-639.547 133,-630.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-630.529 133,-620.529 129.5,-630.529 136.5,-630.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159291136 -->\n",
       "<g class=\"node\" id=\"node15\"><title>139666159291136</title>\n",
       "<polygon fill=\"none\" points=\"26,-511.5 26,-547.5 240,-547.5 240,-511.5 26,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-525.8\">max_pooling2d_30: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 139666159291024&#45;&gt;139666159291136 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>139666159291024-&gt;139666159291136</title>\n",
       "<path d=\"M133,-584.313C133,-576.289 133,-566.547 133,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-557.529 133,-547.529 129.5,-557.529 136.5,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159291304 -->\n",
       "<g class=\"node\" id=\"node16\"><title>139666159291304</title>\n",
       "<polygon fill=\"none\" points=\"75,-438.5 75,-474.5 191,-474.5 191,-438.5 75,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-452.8\">flatten_15: Flatten</text>\n",
       "</g>\n",
       "<!-- 139666159291136&#45;&gt;139666159291304 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>139666159291136-&gt;139666159291304</title>\n",
       "<path d=\"M133,-511.313C133,-503.289 133,-493.547 133,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-484.529 133,-474.529 129.5,-484.529 136.5,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159291416 -->\n",
       "<g class=\"node\" id=\"node17\"><title>139666159291416</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 266,-401.5 266,-365.5 0,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-379.8\">batch_normalization_74: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139666159291304&#45;&gt;139666159291416 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>139666159291304-&gt;139666159291416</title>\n",
       "<path d=\"M133,-438.313C133,-430.289 133,-420.547 133,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-411.529 133,-401.529 129.5,-411.529 136.5,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159291696 -->\n",
       "<g class=\"node\" id=\"node18\"><title>139666159291696</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-292.5 78.5,-328.5 187.5,-328.5 187.5,-292.5 78.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-306.8\">dense_29: Dense</text>\n",
       "</g>\n",
       "<!-- 139666159291416&#45;&gt;139666159291696 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>139666159291416-&gt;139666159291696</title>\n",
       "<path d=\"M133,-365.313C133,-357.289 133,-347.547 133,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-338.529 133,-328.529 129.5,-338.529 136.5,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159292032 -->\n",
       "<g class=\"node\" id=\"node19\"><title>139666159292032</title>\n",
       "<polygon fill=\"none\" points=\"45.5,-219.5 45.5,-255.5 220.5,-255.5 220.5,-219.5 45.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-233.8\">leaky_re_lu_75: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 139666159291696&#45;&gt;139666159292032 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>139666159291696-&gt;139666159292032</title>\n",
       "<path d=\"M133,-292.313C133,-284.289 133,-274.547 133,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-265.529 133,-255.529 129.5,-265.529 136.5,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159292088 -->\n",
       "<g class=\"node\" id=\"node20\"><title>139666159292088</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 266,-182.5 266,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-160.8\">batch_normalization_75: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139666159292032&#45;&gt;139666159292088 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>139666159292032-&gt;139666159292088</title>\n",
       "<path d=\"M133,-219.313C133,-211.289 133,-201.547 133,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-192.529 133,-182.529 129.5,-192.529 136.5,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159292368 -->\n",
       "<g class=\"node\" id=\"node21\"><title>139666159292368</title>\n",
       "<polygon fill=\"none\" points=\"67.5,-73.5 67.5,-109.5 198.5,-109.5 198.5,-73.5 67.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-87.8\">dropout_15: Dropout</text>\n",
       "</g>\n",
       "<!-- 139666159292088&#45;&gt;139666159292368 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>139666159292088-&gt;139666159292368</title>\n",
       "<path d=\"M133,-146.313C133,-138.289 133,-128.547 133,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-119.529 133,-109.529 129.5,-119.529 136.5,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139666159284296 -->\n",
       "<g class=\"node\" id=\"node22\"><title>139666159284296</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-0.5 78.5,-36.5 187.5,-36.5 187.5,-0.5 78.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-14.8\">dense_30: Dense</text>\n",
       "</g>\n",
       "<!-- 139666159292368&#45;&gt;139666159284296 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>139666159292368-&gt;139666159284296</title>\n",
       "<path d=\"M133,-73.3129C133,-65.2895 133,-55.5475 133,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"136.5,-46.5288 133,-36.5288 129.5,-46.5289 136.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix for python3\n",
    "import pydotplus as pydot\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(models[0], to_file='model.png')\n",
    "\n",
    "# visualize our model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(models[0]).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save weights if model is accurate enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the weights for every classifier\n",
    "for i, m in enumerate(models):\n",
    "    m.save_weights(\"weights_leaky_relu_model_0.23_\"+str(i)+'.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
